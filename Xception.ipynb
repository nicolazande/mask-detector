{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- visualizzo piu output a schermo insieme ------------------------------\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "#-------------------------- scelgo se usare CPU o GPU ------------------------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # CPU = -1 , GPU = 0\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "elif cpus:\n",
    "    try:\n",
    "        logical_cpus= tf.config.experimental.list_logical_devices('CPU')\n",
    "        print(len(cpus), \"Physical CPU,\", len(logical_cpus), \"Logical CPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "#-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------serve per dividere in sottocartelle il training set-------------------\n",
    "import json, shutil, os\n",
    "\n",
    "with open('MaskDataset\\\\train_gt.json', 'r') as labels:\n",
    "    data=labels.read()\n",
    "#----------- scegli cartella immagini miste ------------------\n",
    "imm_source= os.path.join(cwd, 'MaskDataset\\\\training')\n",
    "#----------- nome dataset organizzato ------------------------\n",
    "data_dir=os.path.join(cwd,'MaskDataset\\\\dataset')\n",
    "#----------- se non ce ordino tutto --------------------------\n",
    "if not os.path.exists(data_dir):\n",
    "    dir0 = os.path.join(data_dir, '0')\n",
    "    dir1 = os.path.join(data_dir, '1')\n",
    "    dir2 = os.path.join(data_dir, '2')\n",
    "    os.mkdir(data_dir)\n",
    "    os.mkdir(dir0)\n",
    "    os.mkdir(dir1)\n",
    "    os.mkdir(dir2)\n",
    "    #leggo i label dal file json\n",
    "    obj = json.loads(data)\n",
    "    #ciclo su tutti i label e sposto immagini in cartelle separate\n",
    "    for i in obj:\n",
    "        if obj[i]==0:\n",
    "            shutil.copy(os.path.join(imm_source,i), dir0,  follow_symlinks=True)\n",
    "        elif obj[i]==1:\n",
    "            shutil.copy(os.path.join(imm_source,i), dir1,  follow_symlinks=True)\n",
    "        elif obj[i]==2:\n",
    "            shutil.copy(os.path.join(imm_source,i), dir2,  follow_symlinks=True)\n",
    "\n",
    "    labels.close()\n",
    "    \n",
    "#------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------ creo i data generator -------------------\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# scegli se applicare data augmentation (solo su training)\n",
    "apply_data_augmentation = True\n",
    "\n",
    "#---------------- training ----------------------------------------\n",
    "if apply_data_augmentation:\n",
    "    train_data_gen = ImageDataGenerator(validation_split=0.2,\n",
    "                                        rotation_range=30,\n",
    "                                        #brightness_range=[0.8,1.2],\n",
    "                                        width_shift_range=0.2,\n",
    "                                        height_shift_range=0.2,\n",
    "                                        zoom_range=0.15,\n",
    "                                        shear_range=0.15,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=False,\n",
    "                                        fill_mode='nearest',\n",
    "                                        #cval=0,\n",
    "                                        rescale=1./255)\n",
    "# datagenerator senza augmentation\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(validation_split=0.2,\n",
    "                                        rescale=1./255)\n",
    "    \n",
    "#------------------- validation -----------------------------------   \n",
    "validation_data_gen = ImageDataGenerator(validation_split=0.2,\n",
    "                                         rescale=1./255)\n",
    "#------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------- Creo generators da file ------------------------------------\n",
    "source_dir = os.path.join(cwd,'MaskDataset')\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# dimensione immagini (VGG16 vuole 224x224)\n",
    "img_h=299\n",
    "img_w=299\n",
    "\n",
    "#ho tre calssi: 0, 1, 2\n",
    "num_classes=3\n",
    "\n",
    "# usa se vuoi assegnare label diversi\n",
    "decide_class_indices = False\n",
    "if decide_class_indices:\n",
    "    classes = ['0', # no maschera\n",
    "               '1', # tutti maschera\n",
    "               '2'] # alcuni maschera\n",
    "else:\n",
    "    classes=None\n",
    "\n",
    "# Training set\n",
    "dataset_dir = os.path.join(source_dir, 'dataset')\n",
    "train_gen = train_data_gen.flow_from_directory(dataset_dir,\n",
    "                                               target_size=(img_h, img_w),\n",
    "                                               subset='training',\n",
    "                                               batch_size=bs,\n",
    "                                               classes=classes,\n",
    "                                               color_mode=\"rgb\",\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               seed=SEED)\n",
    "\n",
    "# Validation set\n",
    "validation_gen = validation_data_gen.flow_from_directory(dataset_dir,\n",
    "                                                         target_size=(img_h, img_w),\n",
    "                                                         subset='validation',\n",
    "                                                         batch_size=bs,\n",
    "                                                         classes=classes,\n",
    "                                                         color_mode=\"rgb\",\n",
    "                                                         class_mode='categorical',\n",
    "                                                         shuffle=False,\n",
    "                                                         seed=SEED)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------- Creo dataset training -------------------------------------------------------------------\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "#---------------------- Creo dataset validation -----------------------------------------------------------------\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: validation_gen, \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "\n",
    "valid_dataset = valid_dataset.repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base = tf.keras.applications.Xception(weights=\"imagenet\",\n",
    "                                      include_top=False,\n",
    "                                      input_shape=(img_h, img_w, 3))\n",
    "\n",
    "#aggiungo i FC layer\n",
    "model = tf.keras.Sequential()\n",
    "model.add(base)\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Dense(256, activation=('relu')))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units=num_classes, activation=('softmax')))\n",
    "\n",
    "#------------------- scelgo se caricare i pesi ------------------------------\n",
    "load_weights = True\n",
    "model_name = 'model_01'\n",
    "model_dir = os.path.join(cwd, 'Xception')\n",
    "\n",
    "if load_weights:            \n",
    "    model.load_weights(os.path.join(model_dir, model_name))\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "#frizzo tutta la base\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- classe per cambiare lr ----------------------\n",
    "class CLR(tf.keras.callbacks.Callback):\n",
    "    #costruttore (devo passargli schedule)\n",
    "    def __init__(self, schedule):\n",
    "        super(CLR, self).__init__()\n",
    "        #attributo classe = input\n",
    "        self.schedule = schedule\n",
    "    #chiama a ogni epoch\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        #se non ha lr --> errore\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('non hai settato lr')\n",
    "        # Cout << get(lr)\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # lookup table per settare lr\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # cin >> set(lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ preparo il modello per visualizzare i risultati -------------------------------\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# lista di callbacks che vado a usare\n",
    "callbacks = []\n",
    "\n",
    "# --------------- definisco early stopping (non mi va) -----------------\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "#--------------------------- lookup table per lr (standard)------------- \n",
    "\n",
    "LUT_STD = []\n",
    "\n",
    "#------------------- funzione per passare lr ---------------------------\n",
    "def get_lr_std(epoch, lr):\n",
    "    # ritorna lr corrente\n",
    "    if epoch < LUT_STD[0][0]:\n",
    "        return LUT_STD[0][1]\n",
    "    elif epoch > LUT_STD[len(LUT_STD)-1][0]:\n",
    "        # finito STD passo a FT\n",
    "        return LUT_STD[len(LUT_STD)-1][1]\n",
    "    # lookup table --> lr(epoch): cambia solo ai target\n",
    "    for i in range(len(LUT_STD)):\n",
    "        if epoch == LUT_STD[i][0]:\n",
    "            print(\"\\nnuovo lr: \"+str(LUT_STD[i][1]))\n",
    "            return LUT_STD[i][1]\n",
    "    #se non specificato ritorno current\n",
    "    return lr\n",
    "\n",
    "# aggiungo a lista callbacks\n",
    "callbacks.append(CLR(get_lr_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- setto parametri di ottimizzazione ------------------------------\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Compilo il Modello\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------ start fit model ------------------------------\n",
    "EP = 12\n",
    "\n",
    "LUT_STD = [(0, 1e-6),\n",
    "           (6, 1e-8)]\n",
    "\n",
    "for layer in base.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "H = model.fit(x=train_dataset,\n",
    "              epochs=EP,\n",
    "              steps_per_epoch=len(train_gen),\n",
    "              validation_data=valid_dataset,\n",
    "              validation_steps=len(train_gen), \n",
    "              callbacks=callbacks)\n",
    "\n",
    "finetuning = False\n",
    "\n",
    "if finetuning:\n",
    "    \n",
    "    LUT_STD = [(0, 1e-4),\n",
    "               (4, 1e-6),\n",
    "               (8, 1e-8)]\n",
    "    \n",
    "    for layer in base.layers[-6:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    EP = 8\n",
    "    H = model.fit(x=train_dataset,\n",
    "                  epochs=EP,\n",
    "                  steps_per_epoch=len(train_gen),\n",
    "                  validation_data=valid_dataset,\n",
    "                  validation_steps=len(train_gen), \n",
    "                  callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    metrics = ['accuracy']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "              linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "\n",
    "    if metric == 'accuracy':\n",
    "        plt.ylim([0.8,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "plot_metrics(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- salvo i pesi del modello -------------------------------------------------------\n",
    "save_weights = True\n",
    "\n",
    "if save_weights:\n",
    "    model_name = 'model_01'\n",
    "    model_dir = os.path.join(cwd, 'Xception')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)    \n",
    "    model.save_weights(os.path.join(model_dir, model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory dove salvoi i risultati (di entrambi i modelli)\n",
    "results_dir = os.path.join(cwd, 'results')\n",
    "if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "# directori risultati transfer_model\n",
    "results_dir = os.path.join(results_dir, 'Xception')\n",
    "if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "#------------ funzione per formattare il file csv ---------------------------------------------\n",
    "def create_csv(results, results_dir=results_dir):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- scorro le immagini di test e faccio predizioni --------------------------------------\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "\n",
    "imm_source = os.path.join(cwd, 'MaskDataset\\\\test')\n",
    "image_filenames = next(os.walk(imm_source))[2]\n",
    "# scegli tra questi label per le predizioni\n",
    "labels=['0', '1', '2']\n",
    "\n",
    "results = {}\n",
    "for image_name in image_filenames:   \n",
    "    img = image.load_img(os.path.join(imm_source, image_name), target_size=(img_h, img_w))    \n",
    "    img_tensor = image.img_to_array(img)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "    img_tensor /= 255.    \n",
    "    pred = model.predict(img_tensor)\n",
    "    results[image_name] = labels[np.argmax(pred)]\n",
    "    \n",
    "create_csv(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
